# -*- coding: utf-8 -*-
"""Good code 13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZI1mnVUfdJG_egnLxwaxtLttvTN4mHgs

# **Importing the libraries**
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""# **Importing dataset**"""

dataset = pd.read_csv('heart_cleveland_upload.csv')
dataset.head()

# renaming features to proper name
dataset.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',
       'exercise_induced_angina', 'st_depression', 'st_slope', 'VCA', 'thallium', 'target']

dataset.head()

"""# **Data Preparation and Preprocessing**"""

# converting features to categorical features

dataset['chest_pain_type'][dataset['chest_pain_type'] == 0] = 'typical angina'
dataset['chest_pain_type'][dataset['chest_pain_type'] == 1] = 'atypical angina'
dataset['chest_pain_type'][dataset['chest_pain_type'] == 2] = 'non-anginal pain'
dataset['chest_pain_type'][dataset['chest_pain_type'] == 3] = 'asymptomatic'



dataset['rest_ecg'][dataset['rest_ecg'] == 0] = 'normal'
dataset['rest_ecg'][dataset['rest_ecg'] == 1] = 'ST-T wave abnormality'
dataset['rest_ecg'][dataset['rest_ecg'] == 2] = 'left ventricular hypertrophy'


#dataset['st_slope'][dataset['st_slope'] == 0] = 'normal'
dataset['st_slope'][dataset['st_slope'] == 0] = 'upsloping'
dataset['st_slope'][dataset['st_slope'] == 1] = 'flat'
dataset['st_slope'][dataset['st_slope'] == 2] = 'downsloping'

dataset["sex"] = dataset.sex.apply(lambda  x:'male' if x==1 else 'female')

dataset['VCA'][dataset['VCA'] == 0] ='zero'
dataset['VCA'][dataset['VCA'] == 1] ='one'
dataset['VCA'][dataset['VCA'] == 2] ='two'
dataset['VCA'][dataset['VCA'] == 3] ='three'

dataset['thallium'][dataset['thallium'] == 0] ='normal'
dataset['thallium'][dataset['thallium'] == 1] ='fixed defect'
dataset['thallium'][dataset['thallium'] == 2] ='reversible defect'

# checking the top 5 entries of dataset after feature encoding
dataset.head()

X1 = dataset.iloc[:, :-1].values
y1 = dataset.iloc[:, -1].values
print(type(X1))
print(X1)
print(y1)

"""**Handling Missing data**"""

print(dataset.shape)
dataset.dropna(inplace=True)
print(dataset.shape)
dataset.isnull().sum()

"""#**Exploratory Data Analysis (EDA)**

EDA is the process of figuring out what the data can tell us and we use EDA to find patterns, relationships, or anomalies to inform our subsequent analysis.
"""

# summary statistics of numerical columns
dataset.describe(include =[np.number])

# summary statistics of categorical columns
dataset.describe(include =[np.object])

"""**Distribution of Heart disease (target variable)**"""

# Plotting attrition of patients
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=(14,6))

ax1 = dataset['target'].value_counts().plot.pie( x="Heart disease" ,y ='no.of patients', 
                   autopct = "%1.0f%%",labels=["Heart Disease","Normal"], startangle = 60,ax=ax1);
ax1.set(title = 'Percentage of Heart disease patients in Dataset')

ax2 = dataset["target"].value_counts().plot(kind="barh" ,ax =ax2)
for i,j in enumerate(dataset["target"].value_counts().values):
    ax2.text(.5,i,j,fontsize=12)
ax2.set(title = 'No. of Heart disease patients in Dataset')
plt.show()

"""The dataset is balanced having 137 heart disease patients and 160 normal patients

**Checking Gender & Agewise Distribution**
"""

import seaborn as sns
plt.figure(figsize=(18,12))
plt.subplot(221)
dataset["sex"].value_counts().plot.pie(autopct = "%1.0f%%", colors = sns.color_palette("prism",5),startangle = 60,labels=["Male","Female"],

wedgeprops={"linewidth":2, "edgecolor":"k"}, explode=[.1,.1], shadow =True) 
plt.title("Distribution of Gender")
plt.subplot(222)
ax= sns.distplot(dataset['age'], rug=True)
plt.title("Age wise distribution")
plt.show()

"""As we can see from above plot, in this dataset males percentage is higher than females where as average age of patients is around 58."""

attr_1=dataset[dataset['target']==1]
attr_0=dataset[dataset['target']==0]
fig = plt.figure(figsize=(15,5))
ax1 = plt.subplot2grid((1,2),(0,0))
sns.distplot(attr_0['age'])
plt.title('AGE DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold')

ax1 = plt.subplot2grid((1,2),(0,1))
sns.countplot(attr_0['sex'], palette='viridis')
plt.title('GENDER DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold' )
plt.show()

fig = plt.figure(figsize=(15,5))
ax1 = plt.subplot2grid((1,2),(0,0))
sns.distplot(attr_1['age'])
plt.title('AGE DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold')

ax1 = plt.subplot2grid((1,2),(0,1))
sns.countplot(attr_1['sex'], palette='viridis')
plt.title('GENDER DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold' )
plt.show()

"""As we can see from above plot **Male** patients accounts for heart disease in comparison to females whereas mean age for heart disease patients is around 59 years

**Distribution of Chest Pain Type**
"""

fig, ax = plt.subplots(figsize=(10,4))

# Horizontal Bar Plot
title_cnt=dataset.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()
mn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('lightskyblue')
mn[3].set_color('crimson')


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('Chest Pain type Distribution',
             loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')

# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),
            fontsize=10, fontweight='bold', color='grey')
plt.yticks(weight='bold')
plt.xticks(weight='bold')
# Show Plot
plt.show()


fig, ax = plt.subplots(figsize=(10,4))

# Horizontal Bar Plot
title_cnt=attr_1.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()
mn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('red')
mn[3].set_color('blue')


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('Chest Pain type Distribution of Heart patients',
             loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')


# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),
            fontsize=10, fontweight='bold', color='grey')
plt.yticks(weight='bold')
plt.xticks(weight='bold')
# Show Plot
plt.show()

#Exploring the Heart Disease patients based on Chest Pain Type

plot_criteria= ['chest_pain_type', 'target']
cm = sns.light_palette("red", as_cmap=True)
(round (pd.crosstab( dataset[plot_criteria[0]], dataset[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)

"""As we can see from above plot 75% of the chest pain type of the heart disease patients have asymptomatic chest pain.

Asymptomatic heart attacks medically known as silent myocardial infarction (SMI) annually accounts for around 45-50% of morbidities due to cardiac ailments and even premature deaths in India. The incidences among middle aged people experiencing SMI is twice likely to develop in males than females. The symptoms of SMI being very mild in comparison to an actual heart attack; it is described as a silent killer. Unlike the symptoms in a normal heart attack which includes extreme chest pain, stabbing pain in the arms, neck & jaw, sudden shortness of breath, sweating and dizziness, the symptoms of SMI are very brief and hence confused with regular discomfort and most often ignored.

**Distribution of Rest ECG**
"""

fig, ax = plt.subplots(figsize=(10,4))

# Horizontal Bar Plot
title_cnt = dataset.rest_ecg.value_counts().sort_values(ascending=False).reset_index()
mn = ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('lightskyblue')
mn[2].set_color('crimson')


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('Rest ECG Distribution',
             loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')


# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),
            fontsize=10, fontweight ='bold', color ='grey')
plt.yticks(weight ='bold')
plt.xticks(weight ='bold')
# Show Plot
plt.show()


fig, ax = plt.subplots(figsize =(10,4))

# Horizontal Bar Plot
title_cnt=attr_1.rest_ecg.value_counts().sort_values(ascending=False).reset_index()
mn = ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('red')
mn[2].set_color('blue')


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b =True, color ='grey', linestyle ='-.', linewidth =1, alpha =0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('Rest ECG Distribution of Heart patients',
             loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')


# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+ 10, i.get_y()+ 0.5, str(round((i.get_width()), 2)),
            fontsize= 10, fontweight= 'bold', color= 'grey')
plt.yticks(weight= 'bold')
plt.xticks(weight= 'bold')
# Show Plot
plt.show()

#Exploring the Heart Disease patients based on REST ECG
plot_criteria= ['rest_ecg', 'target']
cm = sns.light_palette("red", as_cmap=True)
(round( pd.crosstab( dataset[plot_criteria[0]], dataset[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)

"""An electrocardiogram records the electrical signals in your heart. It's a common test used to detect heart problems and monitor the heart's status in many situations. Electrocardiograms — also called ECGs or EKGs, but ECG has limits. It measures heart rate and rhythm—but it doesn’t necessarily show blockages in the arteries.Thats why in this dataset around 40% heart disease patients have normal ECG"""

fig, ax = plt.subplots(figsize=(10,4))

# Horizontal Bar Plot
title_cnt = dataset.st_slope.value_counts().sort_values(ascending=False).reset_index()
mn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('lightskyblue')
mn[2].set_color('crimson') #3


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('ST Slope Distribution', loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')

# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),
            fontsize=10, fontweight='bold', color='grey')
plt.yticks(weight='bold')
plt.xticks(weight='bold')
# Show Plot
plt.show()

fig, ax = plt.subplots(figsize=(10,4))

# Horizontal Bar Plot
title_cnt=attr_1.st_slope.value_counts().sort_values(ascending=False).reset_index()
mn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')
mn[0].set_color('red')
mn[2].set_color('blue') #3


# Remove axes splines
for s in ['top','bottom','left','right']:
    ax.spines[s].set_visible(False)

# Remove x,y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')

# Add padding between axes and labels
ax.xaxis.set_tick_params(pad=5)
ax.yaxis.set_tick_params(pad=10)

# Add x,y gridlines
ax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)

# Show top values 
ax.invert_yaxis()

# Add Plot Title
ax.set_title('ST Slope Distribution of Heart patients',
             loc='center', pad=10, fontsize=16)
plt.yticks(weight='bold')


# Add annotation to bars
for i in ax.patches:
    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),
            fontsize=10, fontweight='bold', color='grey')
plt.yticks(weight='bold')
plt.xticks(weight='bold')
# Show Plot
plt.show()

#Exploring the Heart Disease patients based on ST Slope
plot_criteria= ['st_slope', 'target']
cm = sns.light_palette("red", as_cmap=True)
(round( pd.crosstab( dataset[plot_criteria[0]], dataset[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)

"""The ST segment /heart rate slope (ST/HR slope), has been proposed as a more accurate ECG criterion for diagnosing significant coronary artery disease (CAD) in most of the research papers.

As we can see from above plot upsloping is positive sign as **64%** of the **normal patients have upslope** where as **64.96% heart patients** have flat sloping.

**Distribution of Numerical features**
"""

sns.pairplot(dataset, hue = 'target', vars = ['age', 'resting_blood_pressure', 'cholesterol'] )

"""From the above plot it is clear that as the age increases chances of heart disease increases"""

sns.scatterplot(x = 'resting_blood_pressure', y = 'age', hue = 'target', data = dataset)

sns.scatterplot(x = 'resting_blood_pressure', y = 'cholesterol', hue = 'target', data = dataset)

"""From the above plots we can see outliers clearly as for some of the patients.

# **Encoding categorical variables**
"""

#Dummy encoding variable


dataset = pd.get_dummies(dataset, drop_first=True)
#dataset.head()
dataset.head()

dataset.shape

X = dataset.drop(['target'],axis=1)
y = dataset['target']

"""# **Train Test Split**

**Splitting the dataset into Training set and Test set**
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size = 0.2, random_state=0)

X_train_std = X_train
X_test_std = X_test
y_train_std = y_train
y_test_std = y_test

X_train_norm = X_train
X_test_norm = X_test
y_train_norm = y_train
y_test_norm = y_test

print(type(X_train))
X_train.head()

y_train.head()

print('------------Training Set------------------')
print(X_train.shape)
print(y_train.shape)

print('------------Test Set------------------')
print(X_test.shape)
print(y_test.shape)

#print(y_train)

#print(X_test)

#print(y_test)

"""# Feature Scaling

**Standard Scaling #standardization**
"""

''' # not needed
X_train = X_train.to_numpy()
y_train = y_train.to_numpy()
X_test = X_test.to_numpy()
y_test = y_test.to_numpy()
print(type(X_train))
'''

# import module
from sklearn.preprocessing import StandardScaler

# compute required values
scaler1 = StandardScaler()

X_train_std[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler1.fit_transform(X_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])
X_train_std.head()

X_test_std[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler1.transform(X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])
X_test_std.head()

"""**Minmax Scaling #Normalization**"""

# import module
from sklearn.preprocessing import MinMaxScaler

# compute required values
scaler2 = MinMaxScaler()

X_train_norm[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler2.fit_transform(X_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])
X_train_norm.head()

X_test_norm[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler2.transform(X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])
X_test_norm.head()

"""# Classification

**Importing Evaluation metrics**
"""

from sklearn.metrics import *

"""### Cross Validation"""

pip install logitboost

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,VotingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.multiclass import OneVsRestClassifier
from logitboost import LogitBoost
from sklearn.svm import SVC 
import xgboost as xgb

from sklearn import model_selection
from sklearn.model_selection import cross_val_score

# function initializing machine learning models
def GetModels():
    Models = []
    Models.append(('LR_L2'   , LogisticRegression(penalty='l2')))

    Models.append(('CART' , DecisionTreeClassifier()))
    #Models.append(('Categorical NB'   , CategoricalNB()))
    Models.append(('Gaussian NB'   , GaussianNB()))
    Models.append(('SVM Linear'  , SVC(kernel='linear',gamma='auto',probability=True)))
    Models.append(('SVM RBF'  , SVC(kernel='rbf',gamma='auto',probability=True)))

    Models.append(('RF_Ent100'   , RandomForestClassifier(criterion='entropy',n_estimators=100)))
    Models.append(('RF_Gini100'   , RandomForestClassifier(criterion='gini',n_estimators=100)))

    #Models.append(('Multi-layer Perceptron', MLPClassifier()))
    Models.append(('AdaBoost'   , AdaBoostClassifier()))
    Models.append(('LogitBoost'   , LogitBoost(n_estimators=200)))
    Models.append(('GradientBoostingClassifier' , GradientBoostingClassifier(n_estimators=100,max_features='sqrt')))

    
    
    return Models

# function for performing 10-fold cross validation of all the models
def crossValidation_k10(X_train_std, y_train_std,models):
    # Test options and evaluation metric
    num_folds = 10
    scoring = ('accuracy','recall','precision')
    #seed = 7
    results = []
    names = []
    for name, model in models:
        kfold = model_selection.KFold(n_splits=10)         #, random_state=seed)
        #cv_results = model_selection.cross_val_score(model, X_train, yls:_train, cv=kfold, scoring=scoring)
        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        
        mean_accuracy = cv_results['test_accuracy'].mean()
        std_accuracy = cv_results['test_accuracy'].std()
        mean_recall = cv_results['test_recall'].mean()
        std_recall = cv_results['test_recall'].std()
        mean_precision = cv_results['test_precision'].mean()
        std_precision = cv_results['test_precision'].std()
        print('\n --------------------',name,'----------------------')
        #print(cv_results)
        print('Accuracy (Mean, Std): (', mean_accuracy,',',std_accuracy,')')
        print('Recall (Mean, Std): (', mean_recall,',',std_recall,')')
        print('Precision (Mean, Std): (', mean_precision,',',std_precision,')')
         
        
    return results

models = GetModels()
#results = crossValidation_k10(X_train_std, y_train_std,models)

"""## ***Classification of test dataset***"""

classifier_names=[]
model_accuracy=[]
for name, model in models:
  classifier_test = model
  classifier_test.fit(X_train_std, y_train_std)
  y_pred = classifier_test.predict(X_test_std)
  cm = confusion_matrix(y_test_std, y_pred)
  print('----------------------',name,'-----------------------')
  print('Confusion matrix: \n', cm)
  classifier_names.append(name)
  model_accuracy.append(accuracy_score(y_test_std, y_pred))
  print('Accuracy: ', accuracy_score(y_test_std, y_pred))
  print('Classification Report:\n ', classification_report(y_test_std, y_pred))

fig = plt.figure(figsize = (17, 7))
plt.ylabel('Accuracy',fontweight ='bold', fontsize = 15)
plt.xlabel('Classifiers',fontweight ='bold', fontsize = 15)
plt.title('Classifier accuracy before feature selection',fontweight ='bold', fontsize = 18)

plt.bar(classifier_names, model_accuracy, color ='orange', width = 0.5)
plt.xticks(range(len(classifier_names)), classifier_names, rotation ='vertical')
plt.show()

"""# **Feature Selection with GA**"""

pip install sklearn-genetic

"""Stacking classifier : NB and SVM"""

from sklearn.ensemble import StackingClassifier
#NB = Categorical or gaussian && svm-- rbf/linear
estimators = [('NB', GaussianNB()),('SVM', SVC(kernel='rbf',gamma='auto',probability=True))]

stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(penalty='l2')) 
#LogisticRegression Accuracy: 0.8333333333333334

stack_clf.fit(X_train_std, y_train_std)
'''
y_pred = stack_clf.predict(X_test_std)
cm = confusion_matrix(y_test_std, y_pred)
print('----------------------Stacking classifier-----------------------')
print('Confusion matrix: \n', cm)
print('Accuracy: ', accuracy_score(y_test_std, y_pred))
print('Classification Report:\n ', classification_report(y_test_std, y_pred))
'''

from sklearn.inspection import permutation_importance  #mda
clf = GaussianNB().fit(X_train_std, y_train_std)
result = permutation_importance(clf, X_train_std, y_train_std, n_repeats=10)

feature_labels_all =['age','resting_blood_pressure','cholesterol', 'fasting_blood_sugar', 'max_heart_rate_achieved', 'exercise_induced_angina', 'st_depression', 'sex_male', 'chest_pain_type_atypical angina', 'chest_pain_type_non-anginal pain', 'chest_pain_type_typical angina', 'rest_ecg_left ventricular hypertrophy', 'rest_ecg_normal', 'st_slope_flat', 'st_slope_upsloping', 'VCA_three', 'VCA_two', 'VCA_zero', 'thallium_normal', 'thallium_reversible defect']

importance = result.importances_mean
i=0
for j in importance:
    #print(feature_labels_all[i],":  ",j)
    i+=1
#print(result.importances_mean)
'''
# plot feature importance
fig = plt.figure(figsize = (17, 7))
plt.xlabel('Features', fontweight ='bold', fontsize = 15)
plt.ylabel('Importances', fontweight ='bold', fontsize = 15)
plt.title('Permutation Importance of the features based on NB',fontweight ='bold', fontsize = 18)

plt.bar(feature_labels_all, importance)
plt.xticks(range(len(feature_labels_all)), feature_labels_all, rotation ='vertical')
plt.show()
'''

from sklearn.inspection import permutation_importance  #mda
clf = SVC(kernel='rbf',gamma='auto',probability=True).fit(X_train_std, y_train_std)
result = permutation_importance(clf, X_train_std, y_train_std, n_repeats=10)

feature_labels_all =['age','resting_blood_pressure','cholesterol', 'fasting_blood_sugar', 'max_heart_rate_achieved', 'exercise_induced_angina', 'st_depression', 'sex_male', 'chest_pain_type_atypical angina', 'chest_pain_type_non-anginal pain', 'chest_pain_type_typical angina', 'rest_ecg_left ventricular hypertrophy', 'rest_ecg_normal', 'st_slope_flat', 'st_slope_upsloping', 'VCA_three', 'VCA_two', 'VCA_zero', 'thallium_normal', 'thallium_reversible defect']
importance = result.importances_mean
i=0
for j in importance:
    #print(feature_labels_all[i],":  ",j)
    i+=1
#print(result.importances_mean)
'''
# plot feature importance
fig = plt.figure(figsize = (17, 7))
plt.xlabel('Features', fontweight ='bold', fontsize = 15)
plt.ylabel('Importances', fontweight ='bold', fontsize = 15)
plt.title('Permutation Importance of the features based on SVM',fontweight ='bold', fontsize = 18)
plt.bar(feature_labels_all, importance)
plt.xticks(range(len(feature_labels_all)), feature_labels_all, rotation ='vertical')
plt.show()
'''

from sklearn.inspection import permutation_importance  #mda
result = permutation_importance(stack_clf, X_train_std, y_train_std, n_repeats=10)

feature_labels_all =['age','resting_blood_pressure','cholesterol', 'fasting_blood_sugar', 'max_heart_rate_achieved', 'exercise_induced_angina', 'st_depression', 'sex_male', 'chest_pain_type_atypical angina', 'chest_pain_type_non-anginal pain', 'chest_pain_type_typical angina', 'rest_ecg_left ventricular hypertrophy', 'rest_ecg_normal', 'st_slope_flat', 'st_slope_upsloping', 'VCA_three', 'VCA_two', 'VCA_zero', 'thallium_normal', 'thallium_reversible defect']
importance = result.importances_mean
i=0
for j in importance:
    print(feature_labels_all[i],":  ",j)
    i+=1
#print(result.importances_mean)
# plot feature importance
fig = plt.figure(figsize = (14, 6))

plt.xlabel('Features', fontweight ='bold', fontsize = 15)
plt.ylabel('Importances', fontweight ='bold', fontsize = 15)
plt.title('Permutation Importance of the features based on StackClassifier',fontweight ='bold', fontsize = 18)

plt.bar(feature_labels_all, importance, width=0.6)
plt.xticks(range(len(feature_labels_all)), feature_labels_all, rotation ='vertical')
plt.show()

# GeneticSelectionCV - "CV" itself finds the max no.of best features

from genetic_selection import GeneticSelectionCV

estimator =  DecisionTreeClassifier()
#estimator =  StackingClassifier(estimators=estimators, final_estimator=MLPClassifier())
#estimator = SVC(kernel='rbf',gamma='auto',probability=True)
#estimator = LogisticRegression(penalty='l2')
model = GeneticSelectionCV(
    estimator, cv=5, verbose=0,
    scoring="accuracy", max_features=11,
    n_population=100, crossover_proba=0.5,
    mutation_proba=0.2, n_generations=50,
    crossover_independent_proba=0.1,
    mutation_independent_proba=0.05,
    tournament_size=3, n_gen_no_change=10,
    caching=True, n_jobs=-1) #set to number of cores
model = model.fit(X_train_std, y_train_std)
#print('Features:', X_train_std.columns[model.support_])

"""Features: Index(['exercise_induced_angina', 'chest_pain_type_atypical angina',
       'rest_ecg_normal', 'VCA_zero', 'thallium_normal'],
      dtype='object')
"""

#print(model.generation_scores_)

"""**SELECTED FEATURES : DECISION TREE**
Features: Index(['exercise_induced_angina', 'chest_pain_type_non-anginal pain',
       'rest_ecg_normal', 'VCA_zero', 'thallium_normal'],
      dtype='object')

## Modified dataset : with only selected features
"""

selected_features = ['exercise_induced_angina','chest_pain_type_atypical angina', 'chest_pain_type_non-anginal pain', 'chest_pain_type_typical angina',
       'rest_ecg_left ventricular hypertrophy', 'rest_ecg_normal',
      'VCA_three', 'VCA_two', 'VCA_zero',
      'thallium_normal', 'thallium_reversible defect'] #

X_train_std_fs =  X_train_std[selected_features]
X_test_std_fs = X_test_std[selected_features]

X_train_std_fs.head()

X_test_std_fs.head()

X_train_norm_fs =  X_train_norm[selected_features]
X_test_norm_fs = X_test_norm[selected_features]

"""## Classification with selected features  """

model_accuracy_fs=[]
#classifier_names=[]
for name, model in models:
  classifier_test = model
  classifier_test.fit(X_train_std_fs, y_train_std)
  y_pred = classifier_test.predict(X_test_std_fs) ####
  cm = confusion_matrix(y_test_std, y_pred)
  print('----------------------',name,'-----------------------')
  print('Confusion matrix: \n', cm)
  #classifier_names.append(name)
  model_accuracy_fs.append(accuracy_score(y_test_std, y_pred))
  print('Accuracy: ', accuracy_score(y_test_std, y_pred))
  print('Classification Report:\n ', classification_report(y_test_std, y_pred))

fig = plt.figure(figsize = (17, 7))
plt.ylabel('Accuracy',fontweight ='bold', fontsize = 15)
plt.xlabel('Classifiers',fontweight ='bold', fontsize = 15)
plt.title('Classifier accuracy after feature selection',fontweight ='bold', fontsize = 18)

plt.bar(classifier_names, model_accuracy_fs, color ='maroon', width = 0.5)
plt.xticks(range(len(classifier_names)), classifier_names, rotation ='vertical')
plt.show()

model_accuracy_diff=[] 
for i in range(len(model_accuracy)):
    model_accuracy_diff.append(model_accuracy_fs[i] - model_accuracy[i])
#print(model_accuracy_diff)

# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(17, 8))

#bar1 - model_accuracy
#bar2 -model_accuracy_fs
#bar3 - model_accuracy_diff
 
# Set position of bar on X axis
br1 = np.arange(len(model_accuracy))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, model_accuracy, color ='orange', width = barWidth,
        edgecolor ='grey', label ='original dataset accuracy')
plt.bar(br2, model_accuracy_fs, color ='blue', width = barWidth,
        edgecolor ='grey', label ='FS dataset accuracy')
plt.bar(br3, model_accuracy_diff, color ='maroon', width = barWidth,
        edgecolor ='grey', label ='Difference')
 
# Adding Xticks
plt.xlabel('Classifiers', fontweight ='bold', fontsize = 15)
plt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(classifier_names))],
        classifier_names, rotation ='vertical')
 
plt.legend()
plt.show()

"""## **Feature importances of the selected features**"""

# after fs

stack_clf.fit(X_train_std_fs, y_train_std)
'''
y_pred = stack_clf.predict(X_test_std_fs)
cm = confusion_matrix(y_test_std, y_pred)
print('----------------------Stacking classifier-----------------------')
print('Confusion matrix: \n', cm)
print('Accuracy: ', accuracy_score(y_test_std, y_pred))
print('Classification Report:\n ', classification_report(y_test_std, y_pred))
'''

result = permutation_importance(stack_clf, X_train_std_fs, y_train_std, n_repeats=10)

feature_labels_selected =['exercise_induced_angina','chest_pain_type_atypical angina', 'chest_pain_type_non-anginal pain', 'chest_pain_type_typical angina', 
        'rest_ecg_left ventricular hypertrophy', 'rest_ecg_normal', 
        'VCA_three', 'VCA_two', 'VCA_zero',
        'thallium_normal', 'thallium_reversible defect']
importance = result.importances_mean
i=0
for j in importance:
    print(feature_labels_selected[i],":  ",j)
    i+=1
#print(result.importances_mean)
# plot feature importance
fig = plt.figure(figsize = (14, 6))

plt.xlabel('Features', fontweight ='bold', fontsize = 15)
plt.ylabel('Importances', fontweight ='bold', fontsize = 15)
plt.title('Permutation Importance of the Selected features',fontweight ='bold', fontsize = 18)

plt.bar(feature_labels_selected, importance, width = 0.6)
plt.xticks(range(len(feature_labels_selected)), feature_labels_selected, rotation ='vertical')
plt.show()

'''
#print(zccc)
xTfs = X_train_std
xtfs = X_test_std

xtfs.shape
'''

'''
X_train_std = X_train_std_fs
X_test_std = X_test_std_fs

X_train_std_fs = xTfs
X_test_std_fs = xtfs
'''

X_train_std.shape

X_train_std_fs.shape

X_test_std.shape

X_test_std_fs.shape

"""# **ANN classification**"""

print(X_train_std.shape)
X_train_data = X_train_std.to_numpy()
X_test_data = X_test_std.to_numpy()
print('X_train_std',type(X_train_std),X_train_std.shape)
print('X_train_data',type(X_train_data), X_train_data.shape)
print('X_test_std',type(X_test_std),X_test_std.shape)
print('X_test_data',type(X_test_data),X_test_data.shape)

print(X_train_data.shape)
print(X_train_data[0].shape)
print(X_train_data[0])

print(X_train_std_fs.shape)
X_train_data_fs = X_train_std_fs.to_numpy()
X_test_data_fs = X_test_std_fs.to_numpy()
print('X_train_std_fs',type(X_train_std_fs),X_train_std_fs.shape)
print('X_train_data_fs',type(X_train_data_fs), X_train_data_fs.shape)
print('X_test_std_fs',type(X_test_std_fs),X_test_std_fs.shape)
print('X_test_data_fs',type(X_test_data_fs),X_test_data_fs.shape)

print(X_train_data_fs.shape)
print(X_train_data_fs[0].shape)
print(X_train_data_fs[0])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input,Dense,Dropout,Conv1D,MaxPooling1D,Flatten

EPOCHS =100
y_train_label = y_train_std.to_numpy()

from keras import callbacks
earlystopping = callbacks.EarlyStopping(monitor ="val_accuracy", 
                                        mode ="auto", patience = 15, 
                                        restore_best_weights = True)

#   model.add(Dense(32,activation='relu', name="Dense_1"))
#   model.add(Dropout(0.2))
#   model.add(Dense(16, activation='relu', name="Dense_2"))
#   model.add(Dropout(0.2))
#   model.add(Dense(8, activation='relu', name="Dense_3"))

def build_ann_model(input_shape):
   
    model = Sequential()
    model.add(Input(shape=(input_shape,)))
    model.add(Dense(32,activation='relu', name="Dense_1"))
    model.add(Dropout(0.2))
    model.add(Dense(16, activation='relu', name="Dense_2"))
    model.add(Dropout(0.2))
    model.add(Dense(8, activation='relu', name="Dense_3"))
    '''
    model1 = Sequential()
    model1.add(Input(shape=(input_shape,)))  ####
    model1.add(Dense(25,activation='relu'))
    model1.add(Dropout(0.2))
    model1.add(Dense(13,activation='relu'))
    model1.add(Dropout(0.2))
    '''

    #output layer
    model.add(Dense(1,activation='sigmoid')) 
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) #adam
    return model

model_ann = build_ann_model(X_train_data.shape[1]) #20
model_ann.summary()

history = model_ann.fit(X_train_data, y_train_label, epochs=EPOCHS,
                    batch_size = 32, verbose=1, validation_data=(X_test_std,y_test_std), callbacks = [earlystopping])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_test_label = y_test_std.to_numpy()
[loss, ann_accuracy, precision, recall] = model_ann.evaluate(X_test_data, y_test_label, verbose=0)

print("accuracy: " , ann_accuracy*100)
print("precision: " , precision*100)
print("recall: " , recall*100)
print("F1 score: " , (2*(precision*recall)/(precision+recall))*100)

"""**ANN for feature selected dataset**"""

model_ann_fs = build_ann_model(X_train_data_fs.shape[1]) #11
model_ann_fs.summary()

history_fs = model_ann_fs.fit(X_train_data_fs, y_train_label, epochs=EPOCHS,
                    batch_size = 32, verbose=1, validation_data=(X_test_std_fs,y_test_std), callbacks = [earlystopping])

plt.plot(history_fs.history['accuracy'])
plt.plot(history_fs.history['val_accuracy'])
plt.title('model accuracy after feature selection ')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#y_test_label = y_test_std.to_numpy()
[loss, ann_accuracy_fs, precision_fs, recall_fs] = model_ann_fs.evaluate(X_test_data_fs, y_test_label, verbose=0)

print("accuracy: " , ann_accuracy_fs*100)
print("precision: " , precision_fs*100)
print("recall: " , recall_fs*100)
print("f1-score", (2*(precision_fs*recall_fs)/(precision_fs + recall_fs))*100)
#y_pred = model.predict(X_test_data_reshaped).flatten()

"""# **CNN conv1D Classification**

### **Reshape the dataset from 2D to 3D**

### Original Dataset
"""

sample_size = X_train_data.shape[0] # number of samples in train set
time_steps  = X_train_data.shape[1] # number of features in train set
input_dimension = 1               # each feature is represented by 1 number

X_train_data_reshaped = X_train_data.reshape(sample_size,time_steps,input_dimension)
print("After reshape train data set shape:\n", X_train_data_reshaped.shape)
print("1 Sample shape:\n",X_train_data_reshaped[0].shape)
print("An example sample:\n", X_train_data_reshaped[0])

X_test_data_reshaped = X_test_data.reshape(X_test_data.shape[0],X_test_data.shape[1],1)
print("After reshape test data set shape:\n", X_test_data_reshaped.shape)
print("1 Sample shape:\n",X_test_data_reshaped[0].shape)
print("An example sample:\n", X_test_data_reshaped[0])

"""### Featured Selected dataset"""

sample_size = X_train_data_fs.shape[0] # number of samples in train set
time_steps  = X_train_data_fs.shape[1] # number of features in train set
input_dimension = 1               # each feature is represented by 1 number

X_train_data_fs_reshaped = X_train_data_fs.reshape(sample_size,time_steps,input_dimension)
print("After reshape train data set shape:\n", X_train_data_fs_reshaped.shape)
print("1 Sample shape:\n",X_train_data_fs_reshaped[0].shape)
print("An example sample:\n", X_train_data_fs_reshaped[0])

X_test_data_fs_reshaped = X_test_data_fs.reshape(X_test_data_fs.shape[0],X_test_data_fs.shape[1],1)
print("After reshape test data set shape:\n", X_test_data_fs_reshaped.shape)
print("1 Sample shape:\n",X_test_data_fs_reshaped[0].shape)
print("An example sample:\n", X_test_data_fs_reshaped[0])

"""## Conv1D model"""

def build_conv1D_model(no_of_features):
    #n_timesteps = X_train_data_reshaped.shape[1] #20
    n_timesteps = no_of_features #20 #11
    n_features  = X_train_data_reshaped.shape[2] #1 
   
    model = Sequential(name="model_conv1D")
    model.add(Input(shape=(n_timesteps,n_features)))

    model.add(Conv1D(filters=8, kernel_size=6, activation='relu', name="Conv1D_1"))
    model.add(Dropout(0.2))
    
    model.add(Conv1D(filters=16, kernel_size=4, activation='relu', name="Conv1D_2"))
    model.add(Dropout(0.2))

    model.add(Conv1D(filters=32, kernel_size=2, activation='relu', name="Conv1D_3"))
    #model.add(Conv1D(filters=16, kernel_size=6, activation='relu', name="Conv1D_4"))
    #model.add(Conv1D(filters=16, kernel_size=4, activation='relu', name="Conv1D_5"))
    
  
    model.add(MaxPooling1D(pool_size=2, name="MaxPooling1D_3"))
    model.add(Flatten())
    model.add(Dense(32, activation='relu', name="Dense_1"))
    model.add(Dense(1, activation='sigmoid'))

    #optimizer = tf.keras.optimizers.RMSprop(0.001) #adam

    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) #adam
    return model

model_conv1D = build_conv1D_model(X_train_data_reshaped.shape[1]) #20
model_conv1D.summary()

history = model_conv1D.fit(X_train_data_reshaped, y_train_label, epochs=EPOCHS,
                    batch_size = 32, verbose=1, validation_data=(X_test_std,y_test_std), callbacks = [earlystopping])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

[loss, cnn_accuracy, precision, recall] = model_conv1D.evaluate(X_test_data_reshaped, y_test_label, verbose=0)

print("accuracy: " , cnn_accuracy*100)
print("precision: " , precision*100)
print("recall: " , recall*100)
print("F1 score: " , (2*(precision*recall)/(precision+recall))*100)
#y_pred = model.predict(X_test_data_reshaped).flatten()

"""### **For dataset with selected features**"""

def build_conv1D_model_fs(no_of_features):
    #n_timesteps = X_train_data_reshaped.shape[1] #20
    n_timesteps = no_of_features #20 #11
    n_features  = X_train_data_reshaped.shape[2] #1 
   
    model = Sequential(name="model_conv1D")
    model.add(Input(shape=(n_timesteps,n_features)))

    model.add(Conv1D(filters=8, kernel_size=6, activation='relu', name="Conv1D_1"))
    model.add(Dropout(0.2))
    
    model.add(Conv1D(filters=16, kernel_size=4, activation='relu', name="Conv1D_2"))
    model.add(Dropout(0.2))

    model.add(Conv1D(filters=32, kernel_size=2, activation='relu', name="Conv1D_3"))
    #model.add(Conv1D(filters=18, kernel_size=3, activation='relu', name="Conv1D_4"))
    #model.add(Conv1D(filters=16, kernel_size=4, activation='relu', name="Conv1D_5"))
    
  
    model.add(MaxPooling1D(pool_size=2, name="MaxPooling1D_3"))
    model.add(Flatten())
    model.add(Dense(32, activation='relu', name="Dense_1"))
    model.add(Dense(1, activation='sigmoid'))

    #optimizer = tf.keras.optimizers.RMSprop(0.001) #adam

    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) #adam
    return model

X_train_data_fs_reshaped.shape[1]

model_conv1D_fs = build_conv1D_model_fs(X_train_data_fs_reshaped.shape[1]) #11
model_conv1D_fs.summary()

history_fs = model_conv1D_fs.fit(X_train_data_fs_reshaped, y_train_label, epochs=EPOCHS,
                    batch_size = 32, verbose=1, validation_data=(X_test_std_fs,y_test_std),callbacks = [earlystopping])

plt.plot(history_fs.history['accuracy'])
plt.plot(history_fs.history['val_accuracy'])
plt.title('model accuracy after feature selection ')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#y_test_label = y_test_std.to_numpy()
[loss, cnn_accuracy_fs, precision_fs, recall_fs] = model_conv1D_fs.evaluate(X_test_data_fs_reshaped, y_test_label, verbose=0)

print("accuracy: " , cnn_accuracy_fs*100)
print("precision: " , precision_fs*100)
print("recall: " , recall_fs*100)
print("f1-score", (2*(precision_fs*recall_fs)/(precision_fs + recall_fs))*100)
#y_pred = model.predict(X_test_data_reshaped).flatten()

print(model_accuracy_fs)

"""# **Visualization**"""

classifier_names.append('ANN')
if(ann_accuracy > ann_accuracy_fs):
    model_accuracy.append(ann_accuracy_fs)
    model_accuracy_fs.append(ann_accuracy)
else:
    model_accuracy.append(ann_accuracy)
    model_accuracy_fs.append(ann_accuracy_fs)

'''
model_accuracy.append(ann_accuracy)
model_accuracy_fs.append(ann_accuracy_fs)
classifier_names.append('ANN')
'''

classifier_names.append('CNN')
if(cnn_accuracy > cnn_accuracy_fs):
    model_accuracy.append(cnn_accuracy_fs)
    model_accuracy_fs.append(cnn_accuracy)
else:    
    model_accuracy.append(cnn_accuracy)
    model_accuracy_fs.append(cnn_accuracy_fs)

'''
model_accuracy.append(cnn_accuracy)
model_accuracy_fs.append(cnn_accuracy_fs)
classifier_names.append('CNN')
'''

fig = plt.figure(figsize = (17, 7))
plt.ylabel('Accuracy',fontweight ='bold', fontsize = 15)
plt.xlabel('Classifiers',fontweight ='bold', fontsize = 15)
plt.title('Classifier accuracy before feature selection',fontweight ='bold', fontsize = 18)

plt.bar(classifier_names, model_accuracy, color ='orange', width = 0.5)
plt.xticks(range(len(classifier_names)), classifier_names, rotation ='vertical')
plt.show()

fig = plt.figure(figsize = (17, 7))
plt.ylabel('Accuracy',fontweight ='bold', fontsize = 15)
plt.xlabel('Classifiers',fontweight ='bold', fontsize = 15)
plt.title('Classifier accuracy after feature selection',fontweight ='bold', fontsize = 18)

plt.bar(classifier_names, model_accuracy_fs, color ='maroon', width = 0.5)
plt.xticks(range(len(classifier_names)), classifier_names, rotation ='vertical')
plt.show()

model_accuracy_diff.append(abs(ann_accuracy_fs - ann_accuracy))

model_accuracy_diff.append(abs(cnn_accuracy_fs - cnn_accuracy))

# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(17, 8))

#bar1 - model_accuracy
#bar2 -model_accuracy_fs
#bar3 - model_accuracy_diff
 
# Set position of bar on X axis
br1 = np.arange(len(model_accuracy))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, model_accuracy, color ='orange', width = barWidth,
        edgecolor ='grey', label ='original dataset accuracy')
plt.bar(br2, model_accuracy_fs, color ='blue', width = barWidth,
        edgecolor ='grey', label ='FS dataset accuracy')
#plt.bar(br3, model_accuracy_diff, color ='maroon', width = barWidth,
#        edgecolor ='grey', label ='Difference in accuracy')
 
# Adding Xticks
plt.xlabel('Classifiers', fontweight ='bold', fontsize = 15)
plt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(classifier_names))],
        classifier_names, rotation ='vertical')
 
plt.legend(loc = 'upper left')
plt.show()